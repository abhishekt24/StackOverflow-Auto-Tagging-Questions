{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict tags on StackOverflow with linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Libraries\n",
    "\n",
    "In this task you will need the following libraries:\n",
    "- [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    "- [Pandas](https://pandas.pydata.org) — a library providing high-performance, easy-to-use data structures and data analysis tools for the Python\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "- [NLTK](http://www.nltk.org) — a platform to work with natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/abhi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this and most of the following assignments you will need to use a list of stop words. It can be downloaded from *nltk*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you will deal with a dataset of post titles from StackOverflow. We are provided a split to 3 sets: *train*, *validation* and *test*. All corpora (except for *test*) contain titles of the posts and corresponding tags (100 tags are available). The *test*  doesn't contain answers. Upload the corpora using *pandas* and look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    data = pd.read_csv(filename, sep='\\t')\n",
    "    data['tags'] = data['tags'].apply(literal_eval)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = read_data('data/train.tsv')\n",
    "validation = read_data('data/validation.tsv')\n",
    "test = pd.read_csv('data/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to draw a stacked dotplot in R?</td>\n",
       "      <td>[r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mysql select all records where a datetime fiel...</td>\n",
       "      <td>[php, mysql]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to terminate windows phone 8.1 app</td>\n",
       "      <td>[c#]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>get current time in a specific country via jquery</td>\n",
       "      <td>[javascript, jquery]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Configuring Tomcat to Use SSL</td>\n",
       "      <td>[java]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Awesome nested set plugin - how to add new chi...</td>\n",
       "      <td>[ruby-on-rails]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How to create map from JSON response in Ruby o...</td>\n",
       "      <td>[ruby, ruby-on-rails-3, json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rspec test if method is called</td>\n",
       "      <td>[ruby]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SpringBoot Catalina LifeCycle Exception</td>\n",
       "      <td>[java, spring, spring-mvc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How to import data from excel to mysql databas...</td>\n",
       "      <td>[php, codeigniter]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                How to draw a stacked dotplot in R?   \n",
       "1  mysql select all records where a datetime fiel...   \n",
       "2             How to terminate windows phone 8.1 app   \n",
       "3  get current time in a specific country via jquery   \n",
       "4                      Configuring Tomcat to Use SSL   \n",
       "5  Awesome nested set plugin - how to add new chi...   \n",
       "6  How to create map from JSON response in Ruby o...   \n",
       "7                     rspec test if method is called   \n",
       "8            SpringBoot Catalina LifeCycle Exception   \n",
       "9  How to import data from excel to mysql databas...   \n",
       "\n",
       "                            tags  \n",
       "0                            [r]  \n",
       "1                   [php, mysql]  \n",
       "2                           [c#]  \n",
       "3           [javascript, jquery]  \n",
       "4                         [java]  \n",
       "5                [ruby-on-rails]  \n",
       "6  [ruby, ruby-on-rails-3, json]  \n",
       "7                         [ruby]  \n",
       "8     [java, spring, spring-mvc]  \n",
       "9             [php, codeigniter]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, *title* column contains titles of the posts and *tags* column contains the tags. It could be noticed that a number of tags for a post is not fixed and could be as many as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more comfortable usage, initialize *X_train*, *X_val*, *X_test*, *y_train*, *y_val*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = train['title'].values, train['tags'].values\n",
    "X_val, y_val = validation['title'].values, validation['tags'].values\n",
    "X_test = test['title'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most known difficulties when working with natural data is that it's unstructured. For example, if you use it \"as is\" and extract tokens just by splitting the titles by whitespaces, you will see that there are many \"weird\" tokens like *3.5?*, *\"Flip*, etc. To prevent the problems, it's usually useful to prepare the data somehow. In this task you'll write a function, which will be also used in the other assignments. \n",
    "\n",
    "**Task 1 (TextPrepare).** Implement the function *text_prepare* following the instructions. After that, run the function *test_test_prepare* to test it on tiny cases and submit it to Coursera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def text_prepare(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower()# lowercase text\n",
    "    text = re.sub(REPLACE_BY_SPACE_RE,' ',text)# replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text=re.sub(BAD_SYMBOLS_RE,'',text)#text = # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    #text=re.sub(STOPWORDS,\"\",text)#text = # delete stopwords from text\n",
    "    #for w in STOPWORDS:\n",
    "    text=' '.join(i for i in text.split() if i not in STOPWORDS)\n",
    "    print (text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_text_prepare():\n",
    "    examples = [\"SQL Server - any equivalent of Excel's CHOOSE function?\",\n",
    "                \"How to free c++ memory vector<int> * arr?\"]\n",
    "    answers = [\"sql server equivalent excels choose function\", \n",
    "               \"free c++ memory vectorint arr\"]\n",
    "    for ex, ans in zip(examples, answers):\n",
    "        if text_prepare(ex) != ans:\n",
    "            return \"Wrong answer for the case: '%s'\" % ex\n",
    "    return 'Basic tests are passed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sql server equivalent excels choose function\n",
      "free c++ memory vectorint arr\n",
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_text_prepare())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlite php readonly\n",
      "creating multiple textboxes dynamically\n",
      "self one prefer javascript\n",
      "save php date string mysql database timestamp\n",
      "fill dropdownlist data xml file aspnet application\n",
      "programmatically trigger jqueryui draggables drag event\n",
      "get value method argument via reflection java\n",
      "knockout mapingfromjs observablearray json object data gets lost\n",
      "facebook connect localhost weird stuff\n",
      "fullcalendar prev next click\n",
      "syntaxerror unexpected token\n",
      "effective way float double comparison\n",
      "gem install rails fails dns error\n",
      "listshuttle component richfaces getting updated\n",
      "laravel responsedownload show images laravel\n",
      "wrong rspec test\n",
      "calendar display using java swing\n",
      "python selenium import regular firefox profile addons\n",
      "random number 2 variables values\n",
      "altering http responses firefox extension\n",
      "start session python web application\n",
      "align radio buttons horizontally django forms\n",
      "count number rows sqlite database\n",
      "wordpress wp_rewrite rules\n",
      "removing sheet excel 2005 using php\n",
      "php fatal error function name must string\n",
      "avoid used another process using filecopy c#\n",
      "php calling class method class name variable string\n",
      "vector iterator dereferencable loop\n",
      "mysql search statement multiple filters\n",
      "php bluetooth rfcomm library\n",
      "alternatives javas scanner class console input\n",
      "php displaying errors\n",
      "check whether two matrixes identical opencv\n",
      "open external links open new tab apart domain\n",
      "registrygetvalue always return null\n",
      "installing eventmachine windows 8\n",
      "calculate throughput\n",
      "want seperate integer part fractional part float number python\n",
      "properly manage tomcat web apps inside eclipse\n",
      "trouble setting headers qnetworkrequest cant understand\n",
      "extend request class laravel 5\n",
      "difference matrix asmatrix r\n",
      "install clickonce without running\n",
      "jquery selected option value contains string\n",
      "error deploying tomcat tomcatmanager status code404 reasonphrasenot found\n",
      "pretty print distances ios\n",
      "error expected unqualifiedid const line 8\n",
      "mocking reflection based calls\n",
      "set blank excel cell using poi\n",
      "xcode code coverage\n",
      "creating interactive bar chart google analytics data\n",
      "mathh compilation error expected declaration specifiers\n",
      "disadvantage object composition class inheritance\n",
      "commongodbmongotimeoutexception timed 10000 ms waiting connect\n",
      "python pandas skip columns reading file\n",
      "unable get property 1 undefined null reference\n",
      "pydeveclipse python configured\n",
      "python socket server receive image\n",
      "event handle\n",
      "ld_library_path ignored android sometimes\n",
      "coin toss javascript html\n",
      "efficient way concatenate strings\n",
      "javalangnullpointerexception trying read files\n",
      "assigning lambda actiont\n",
      "split string list keeping split pattern\n",
      "compile c code windows 64\n",
      "yii condition multiple ors operator precedence\n",
      "show sql queries run rails console\n",
      "change viewstate value dynamically\n",
      "best practice list polymorphic objects c++\n",
      "javascript get values selected checkboxes table\n",
      "python convert hex ascii string raw internal binary string\n",
      "magento multi language rewrites seo fix\n",
      "ul form sending _post\n",
      "showing alert box using php javascript\n",
      "google maps api v3 looping adding polylines map\n",
      "php remove characters last occurrence character string\n",
      "use qt creator visual c++ compiler windows\n",
      "apache spark python lambda\n",
      "adjusting camera visible threejs shape\n",
      "python imaging library show windows\n",
      "open close website using default browser python\n",
      "php security nonce unique form key problem\n",
      "spring batch simplejobrepository example working\n",
      "getting oledbexception value given one required parameters\n",
      "php session deleted page reload\n",
      "vaadin multiple browser windows tabs\n",
      "frustrating uiwebview delegate crash issue\n",
      "load factor capacity hash table\n",
      "format double string show decimal digits necessary\n",
      "round java\n",
      "using usb printer c# without driver api\n",
      "unbalanced parenthesis error regex\n",
      "reading excel file c# causes oledbexception thrown\n",
      "want compare dtrows 1 itemarray 1 tostring 1 2 3 4 work show error\n",
      "javaniocharsetmalformedinputexception input length 1\n",
      "join two unrelated tables using jpa entitymanager\n",
      "ensuring threadsafety static methods c#\n",
      "loop focus input fields inside form\n"
     ]
    }
   ],
   "source": [
    "prepared_questions = []\n",
    "for line in open('data/text_prepare_tests.tsv', encoding='utf-8'):\n",
    "    line = text_prepare(line.strip())\n",
    "    prepared_questions.append(line)\n",
    "text_prepare_results = '\\n'.join(prepared_questions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can preprocess the titles using function *text_prepare* and  making sure that the headers don't have bad symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [text_prepare(x) for x in X_train]\n",
    "X_val = [text_prepare(x) for x in X_val]\n",
    "X_test = [text_prepare(x) for x in X_test]\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['draw stacked dotplot r',\n",
       " 'mysql select records datetime field less specified value',\n",
       " 'terminate windows phone 81 app']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each tag and for each word calculate how many times they occur in the train corpus. \n",
    "\n",
    "**Task 2 (WordsTagsCount).** Find 3 most popular tags and 3 most popular words in the train data and submit the results to earn the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r': 1727, 'php': 13907, 'mysql': 3092, 'c#': 19077, 'javascript': 19078, 'jquery': 7510, 'java': 18661, 'ruby-on-rails': 3344, 'ruby': 2326, 'ruby-on-rails-3': 692, 'json': 2026, 'spring': 1346, 'spring-mvc': 618, 'codeigniter': 786, 'class': 509, 'html': 4668, 'ios': 3256, 'c++': 6469, 'eclipse': 992, 'python': 8940, 'list': 693, 'objective-c': 4338, 'swift': 1465, 'xaml': 438, 'asp.net': 3939, 'wpf': 1289, 'multithreading': 1118, 'image': 672, 'performance': 512, 'twitter-bootstrap': 501, 'linq': 964, 'xml': 1347, 'numpy': 502, 'ajax': 1767, 'django': 1835, 'laravel': 525, 'android': 2818, 'rest': 456, 'asp.net-mvc': 1244, 'web-services': 633, 'string': 1573, 'excel': 443, 'winforms': 1468, 'arrays': 2277, 'c': 3119, 'sockets': 579, 'osx': 490, 'entity-framework': 649, 'mongodb': 350, 'opencv': 401, 'xcode': 900, 'uitableview': 460, 'algorithm': 419, 'python-2.7': 421, 'angularjs': 1353, 'dom': 400, 'swing': 759, '.net': 3872, 'vb.net': 1918, 'google-maps': 408, 'hibernate': 807, 'wordpress': 478, 'iphone': 1909, 'sql': 1272, 'visual-studio': 574, 'linux': 793, 'facebook': 508, 'database': 740, 'file': 582, 'generics': 420, 'visual-studio-2010': 588, 'regex': 1442, 'html5': 842, 'jsp': 680, 'csv': 435, 'forms': 872, 'validation': 558, 'parsing': 403, 'function': 487, 'pandas': 479, 'sorting': 375, 'qt': 451, 'wcf': 389, 'css': 1769, 'date': 560, 'node.js': 771, 'sql-server': 585, 'unit-testing': 449, 'python-3.x': 379, 'loops': 389, 'windows': 838, 'pointers': 350, 'oop': 425, 'datetime': 557, 'servlets': 498, 'session': 415, 'cocoa-touch': 507, 'apache': 441, 'selenium': 431, 'maven': 432}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of all tags from train corpus with their counts.\n",
    "tags_counts = {}\n",
    "# Dictionary of all words from train corpus with their counts.\n",
    "words_counts = {}\n",
    "\n",
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################\n",
    "\n",
    "for sent in X_train:\n",
    "    words=sent.split(\" \")\n",
    "    for x in words:\n",
    "        if x not in words_counts.keys():\n",
    "            words_counts[x]=1\n",
    "        else:\n",
    "            words_counts[x]+=1\n",
    "\n",
    "for y in y_train:\n",
    "    for w in y:\n",
    "        if w not in tags_counts.keys():\n",
    "            tags_counts[w]=1\n",
    "        else:\n",
    "            tags_counts[w]+=1\n",
    "print(tags_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are assuming that *tags_counts* and *words_counts* are dictionaries like `{'some_word_or_tag': frequency}`. After applying the sorting procedure, results will be look like this: `[('most_popular_word_or_tag', frequency), ('less_popular_word_or_tag', frequency), ...]`. The grader gets the results in the following format (two comma-separated strings with line break):\n",
    "\n",
    "    tag1,tag2,tag3\n",
    "    word1,word2,word3\n",
    "\n",
    "Pay attention that in this assignment you should not submit frequencies or some additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_common_tags = sorted(tags_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "most_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming text to a vector\n",
    "\n",
    "Machine Learning algorithms work with numeric data and we cannot use the provided text data \"as is\". There are many ways to transform text data to numeric vectors. In this task you will try to use two of them.\n",
    "\n",
    "#### Bag of words\n",
    "\n",
    "One of the well-known approaches is a *bag-of-words* representation. To create this transformation, follow the steps:\n",
    "1. Find *N* most popular words in train corpus and numerate them. Now we have a dictionary of the most popular words.\n",
    "2. For each title in the corpora create a zero vector with the dimension equals to *N*.\n",
    "3. For each text in the corpora iterate over words which are in the dictionary and increase by 1 the corresponding coordinate.\n",
    "\n",
    "Let's try to do it for a toy example. Imagine that we have *N* = 4 and the list of the most popular words is \n",
    "\n",
    "    ['hi', 'you', 'me', 'are']\n",
    "\n",
    "Then we need to numerate them, for example, like this: \n",
    "\n",
    "    {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n",
    "\n",
    "And we have the text, which we want to transform to the vector:\n",
    "\n",
    "    'hi how are you'\n",
    "\n",
    "For this text we create a corresponding zero vector \n",
    "\n",
    "    [0, 0, 0, 0]\n",
    "    \n",
    "And iterate over all words, and if the word is in the dictionary, we increase the value of the corresponding position in the vector:\n",
    "\n",
    "    'hi':  [1, 0, 0, 0]\n",
    "    'how': [1, 0, 0, 0] # word 'how' is not in our dictionary\n",
    "    'are': [1, 0, 0, 1]\n",
    "    'you': [1, 1, 0, 1]\n",
    "\n",
    "The resulting vector will be \n",
    "\n",
    "    [1, 1, 0, 1]\n",
    "   \n",
    "Implement the described encoding in the function *my_bag_of_words* with the size of the dictionary equals to 5000. To find the most common words use train data. You can test your code using the function *test_my_bag_of_words*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DICT_SIZE = 5000\n",
    "most_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:5000]\n",
    "list1=[word for word, _ in most_common_words]\n",
    "i=0\n",
    "#print(list1)\n",
    "WORDS_TO_INDEX ={list1[i]:i for i in range(0,5000)} ####### YOUR CODE HERE #######\n",
    "#INDEX_TO_WORDS = ####### YOUR CODE HERE #######\n",
    "ALL_WORDS = WORDS_TO_INDEX.keys()\n",
    "#print(WORDS_TO_INDEX)\n",
    "\n",
    "def my_bag_of_words(text, words_to_index, dict_size):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        dict_size: size of the dictionary\n",
    "        \n",
    "        return a vector which is a bag-of-words representation of 'text'\n",
    "    \"\"\"\n",
    "    result_vector = np.zeros(dict_size)\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    text1=text.split(' ')\n",
    "    for w in text1:\n",
    "        if w in words_to_index.keys():\n",
    "            result_vector[words_to_index[w]] =1\n",
    "    print(result_vector)\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_my_bag_of_words():\n",
    "    words_to_index = {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n",
    "    examples = ['hi how are you']\n",
    "    answers = [[1, 1, 0, 1]]\n",
    "    for ex, ans in zip(examples, answers):\n",
    "        if (my_bag_of_words(ex, words_to_index, 4) != ans).any():\n",
    "            return \"Wrong answer for the case: '%s'\" % ex\n",
    "    return 'Basic tests are passed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  0.  1.]\n",
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_my_bag_of_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply the implemented function to all samples (this might take up to a minute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import sparse as sp_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_train])\n",
    "X_val_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_val])\n",
    "X_test_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_test])\n",
    "print('X_train shape ', X_train_mybag.shape)\n",
    "print('X_val shape ', X_val_mybag.shape)\n",
    "print('X_test shape ', X_test_mybag.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might notice, we transform the data to sparse representation, to store the useful information efficiently. There are many [types](https://docs.scipy.org/doc/scipy/reference/sparse.html) of such representations, however sklearn algorithms can work only with [csr](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix) matrix, so we will use this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3 (BagOfWords).** For the 11th row in *X_train_mybag* find how many non-zero elements it has. In this task the answer (variable *non_zero_elements_count*) should be a number, e.g. 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row = X_train_mybag[10].toarray()[0]\n",
    "#print(row)\n",
    "non_zero_elements_count =np.count_nonzero(row) ####### YOUR CODE HERE #######\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "\n",
    "The second approach extends the bag-of-words framework by taking into account total frequencies of words in the corpora. It helps to penalize too frequent words and provide better features space. \n",
    "\n",
    "Implement function *tfidf_features* using class [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) from *scikit-learn*. Use *train* corpus to train a vectorizer. Don't forget to take a look into the arguments that you can pass to it. We suggest that you filter out too rare words (occur less than in 5 titles) and too frequent words (occur more than in 90% of the titles). Also, use bigrams along with unigrams in your vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf_features(X_train, X_val, X_test):\n",
    "    \"\"\"\n",
    "        X_train, X_val, X_test — samples        \n",
    "        return TF-IDF vectorized representation of each sample and vocabulary\n",
    "    \"\"\"\n",
    "    # Create TF-IDF vectorizer with a proper parameters choice\n",
    "    # Fit the vectorizer on the train set\n",
    "    # Transform the train, test, and val sets and return the result\n",
    "    \n",
    "    \n",
    "    tfidf_vectorizer =  TfidfVectorizer(min_df = 5,token_pattern= '(\\S+)')####### YOUR CODE HERE #######\n",
    "    X_train= tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_val= tfidf_vectorizer.transform(X_val)\n",
    "    X_test= tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    \n",
    "    return X_train, X_val, X_test, tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have done text preprocessing, always have a look at the results. Be very careful at this step, because the performance of future models will drastically depend on it. \n",
    "\n",
    "In this case, check whether you have c++ or c# in your vocabulary, as they are obviously important tokens in our tags prediction task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vocab = tfidf_features(X_train, X_val, X_test)\n",
    "tfidf_reversed_vocab = {i:word for word,i in tfidf_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "######### YOUR CODE HERE #############\n",
    "for i in tfidf_reversed_vocab.keys():\n",
    "    if(tfidf_reversed_vocab[i]==\"c++\" or tfidf_reversed_vocab[i]==\"c\" or tfidf_reversed_vocab[i]==\"c#\" ):\n",
    "        print(\"yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can't find it, we need to understand how did it happen that we lost them? It happened during the built-in tokenization of TfidfVectorizer. Luckily, we can influence on this process. Get back to the function above and use '(\\S+)' regexp as a *token_pattern* in the constructor of the vectorizer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use this transormation for the data and check again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE #############\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def n_grams():\n",
    "    vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    X_val_vectorized= vect.transform(X_val)\n",
    "    return X_train_vectorized,X_val_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100000x17778 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 653252 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ng,X_val_ng=n_grams()\n",
    "X_train_ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiLabel classifier\n",
    "\n",
    "As we have noticed before, in this task each example can have multiple tags. To deal with such kind of prediction, we need to transform labels in a binary form and the prediction will be a mask of 0s and 1s. For this purpose it is convenient to use [MultiLabelBinarizer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html) from *sklearn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(classes=sorted(tags_counts.keys()))\n",
    "y_train = mlb.fit_transform(y_train)\n",
    "y_val = mlb.transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function *train_classifier* for training a classifier. In this task we suggest to use One-vs-Rest approach, which is implemented in [OneVsRestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html) class. In this approach *k* classifiers (= number of tags) are trained. As a basic classifier, use [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). It is one of the simplest methods, but often it performs good enough in text classification tasks. It might take some time, because a number of classifiers to train is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(X_train, y_train):\n",
    "    \"\"\"\n",
    "      X_train, y_train — training data\n",
    "      \n",
    "      return: trained classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create and fit LogisticRegression wraped into OneVsRestClassifier.\n",
    "    #clf= LogisticRegression(random_state=0, multi_class='ovr',penalty='l2,C=1.0)\n",
    "    #print(X_train.getnnz())\n",
    "    #print(len(y_train))\n",
    "    clf = OneVsRestClassifier(LogisticRegression(penalty='l1',C=10))\n",
    "    clf.fit(X_train,y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the classifiers for different data transformations: *bag-of-words* and *tf-idf*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_mybag = train_classifier(X_train_mybag, y_train)\n",
    "classifier_tfidf = train_classifier(X_train_tfidf, y_train)\n",
    "classifier_n_grams=train_classifier(X_train_ng,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_predicted_labels_ng=classifier_n_grams.predict(X_val_ng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can create predictions for the data. You will need two types of predictions: labels and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "y_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag)\n",
    "y_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)\n",
    "\n",
    "y_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)\n",
    "y_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)\n",
    "\n",
    "print(y_val_predicted_labels_mybag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take a look at how classifier, which uses TF-IDF, works for a few examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would need to compare the results of different predictions, e.g. to see whether TF-IDF transformation helps or to try different regularization techniques in logistic regression. For all these experiments, we need to setup evaluation procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "To evaluate the results we will use several classification metrics:\n",
    " - [Accuracy](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    " - [F1-score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    " - [Area under ROC-curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
    " - [Area under precision-recall curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score) \n",
    " \n",
    "Make sure you are familiar with all of them. How would you expect the things work for the multi-label scenario? Read about micro/macro/weighted averaging following the sklearn links provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function *print_evaluation_scores* which calculates and prints to stdout:\n",
    " - *accuracy*\n",
    " - *F1-score macro/micro/weighted*\n",
    " - *Precision macro/micro/weighted*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_evaluation_scores(y_val, predicted):\n",
    "\n",
    "    print(accuracy_score(y_val, predicted))\n",
    "    print(f1_score(y_val, predicted, average='weighted'))\n",
    "    print(average_precision_score(y_val, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IN Multilabel classification f-score is more significant evaluation criteria\n",
    "The problem with Multilabel classification is that even if one of the predicted tags is wrong/extra then \n",
    "the whole prediction is considered wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-words\n",
      "0.337566666667\n",
      "0.649509939626\n",
      "0.316057862529\n",
      "Tfidf\n",
      "0.353733333333\n",
      "0.653333183953\n",
      "0.327888523013\n",
      "N-grams\n",
      "0.2806\n",
      "0.6123922307\n",
      "0.298305222005\n"
     ]
    }
   ],
   "source": [
    "print('Bag-of-words')\n",
    "print_evaluation_scores(y_val, y_val_predicted_labels_mybag)\n",
    "print('Tfidf')\n",
    "print_evaluation_scores(y_val, y_val_predicted_labels_tfidf)\n",
    "print('N-grams')\n",
    "print_evaluation_scores(y_val, y_val_predicted_labels_ng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (MultilabelClassification).** Once we have the evaluation set up, we suggest that you experiment a bit with training your classifiers. We will use *F1-score weighted* as an evaluation metric. Our recommendation:\n",
    "- compare the quality of the bag-of-words and TF-IDF approaches and chose one of them.\n",
    "- for the chosen one, try *L1* and *L2*-regularization techniques in Logistic Regression with different coefficients (e.g. C equal to 0.1, 1, 10, 100).\n",
    "\n",
    "You also could try other improvements of the preprocessing / model, if you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions = classifier_tfidf.predict(X_test_tfidf)######### YOUR CODE HERE #############\n",
    "test_pred_inversed = mlb.inverse_transform(test_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is usually a good idea to look at the features (words or n-grams) that are used with the largest weigths in your logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function *print_words_for_tag* to find them. Get back to sklearn documentation on [OneVsRestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html) and [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_words_for_tag(classifier, tag, tags_classes, index_to_words, all_words):\n",
    "    \"\"\"\n",
    "        classifier: trained classifier\n",
    "        tag: particular tag\n",
    "        tags_classes: a list of classes names from MultiLabelBinarizer\n",
    "        index_to_words: index_to_words transformation\n",
    "        all_words: all words in the dictionary\n",
    "        \n",
    "        return nothing, just print top 5 positive and top 5 negative words for current tag\n",
    "    \"\"\"\n",
    "    print('Tag:\\t{}'.format(tag))\n",
    "    \n",
    "    # Extract an estimator from the classifier for the given tag.\n",
    "    # Extract feature coefficients from the estimator. \n",
    "    \n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    est = classifier.estimators_[tags_classes.index(tag)]\n",
    "    top_positive_words = [index_to_words[index] for index in est.coef_.argsort().tolist()[0][-5:]]# top-5 words sorted by the coefficiens.\n",
    "    top_negative_words = [index_to_words[index] for index in est.coef_.argsort().tolist()[0][:5]]# bottom-5 words  sorted by the coefficients.\n",
    "    print('Top positive words:\\t{}'.format(', '.join(top_positive_words)))\n",
    "    print('Top negative words:\\t{}\\n'.format(', '.join(top_negative_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag:\tc\n",
      "Top positive words:\tpthreads, fscanf, malloc, scanf, c\n",
      "Top negative words:\tbegin, php, objective, collection, rails\n",
      "\n",
      "Tag:\tc++\n",
      "Top positive words:\tboost, c++11, stdstring, boostasio, c++\n",
      "Top negative words:\tpretty, php, jquery, resharper, transform\n",
      "\n",
      "Tag:\tlinux\n",
      "Top positive words:\tassume, killed, reaching, nsurlrequest, linux\n",
      "Top negative words:\taspnet, displaying, codeigniter, nokogiri, 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_words_for_tag(classifier_tfidf, 'c', mlb.classes, tfidf_reversed_vocab, ALL_WORDS)\n",
    "print_words_for_tag(classifier_tfidf, 'c++', mlb.classes, tfidf_reversed_vocab, ALL_WORDS)\n",
    "print_words_for_tag(classifier_tfidf, 'linux', mlb.classes, tfidf_reversed_vocab, ALL_WORDS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
